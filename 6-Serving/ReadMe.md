# 模型的服务器部署

当我们将模型训练完毕后，往往需要将模型在生产环境中部署。最常见的方式，是在服务器上提供一个 API，即客户机向服务器的某个 API 发送特定格式的请求，服务器收到请求数据后通过模型进行计算，并返回结果。  
如果仅仅是做一个 Demo，不考虑高并发和性能问题，其实配合 Flask 等 Python 下的 Web 框架就能非常轻松地实现服务器 API。  
不过，如果是在真的实际生产环境中部署，这样的方式就显得力不从心了。这时，TensorFlow 为我们提供了 TensorFlow Serving 这一组件，能够帮助我们在实际生产环境中灵活且高性能地部署机器学习模型。


## TensorFlow Serving 安装



